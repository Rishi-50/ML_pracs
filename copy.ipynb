{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLAINABLE AI: LIME ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the data for modelling\n",
    "\n",
    "y=df['target'].to_frame() # define Y\n",
    "X=df[df.columns.difference(['target'])] # define X\n",
    "\n",
    "X_train, X_test, y_train, y_test = None, None, None, None #Initializing required variables\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Building model - Xgboost\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making prediction with test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Performance Measurement\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# Classifier function for lime explanation\n",
    "classifier_fn = lambda x: model.predict_proba(x).astype(float)\n",
    "\n",
    "# LIME Explainer\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values,\n",
    "                                                  mode='classification',\n",
    "                                                  feature_names=X_train.columns,\n",
    "                                                  class_names=['negative', 'positive'])\n",
    "\n",
    "# Explain a single data point (e.g., 5th index)\n",
    "exp = explainer.explain_instance(X_test.values[5], classifier_fn, num_features=X_test.shape[1])\n",
    "\n",
    "# Show the explanation in a notebook form\n",
    "exp.show_in_notebook(show_all=False)\n",
    "\n",
    "# Draw a bar chart of the above explained data point\n",
    "exp.as_pyplot_figure()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLAINABLE AI: SHAP ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the data for modelling\n",
    "\n",
    "y=df['target'].to_frame() # define Y\n",
    "X=df[df.columns.difference(['target'])] # define X\n",
    "\n",
    "X_train, X_test, y_train, y_test = None, None, None, None #Initializing required variables\n",
    "### Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Building model - Xgboost\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making prediction with test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Performance Measurement\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# LIME Explainer\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Store SHAP values and expected values\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "expected_values = explainer.expected_value\n",
    "\n",
    "# Make a summary plot of feature importance\n",
    "shap.summary_plot(shap_values, X_test, feature_names=X_test.columns)\n",
    "\n",
    "# Make a bar graph plot\n",
    "shap.summary_plot(shap_values[1], X_test, feature_names=X_test.columns, plot_type=\"bar\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection SOM Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install minisom\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from minisom import MiniSom\n",
    "\n",
    "# Loading Data\n",
    "data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Credit_Card_Applications.csv')\n",
    "\n",
    "# Shape of the data:\n",
    "print(\"Shape of the data:\", data.shape)\n",
    "#********************#\n",
    "\n",
    "# Info of the data:\n",
    "print(\"Info of the data:\")\n",
    "print(data.info())\n",
    "#********************#\n",
    "\n",
    "# Defining X variables for the input of SOM\n",
    "X = data.iloc[:, 1:14].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "# Convert X variable into a pandas DataFrame\n",
    "X = pd.DataFrame(X)\n",
    "#********************#\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sc = MinMaxScaler(feature_range=(0, 1))\n",
    "X = sc.fit_transform(X)\n",
    "X = pd.DataFrame(X)\n",
    "\n",
    "# Set the hyperparameters\n",
    "som_grid_rows = 10\n",
    "som_grid_columns = 10\n",
    "iterations = 20000\n",
    "sigma = 1\n",
    "learning_rate = 0.5\n",
    "\n",
    "# Create MiniSom Model\n",
    "som = MiniSom(x=som_grid_rows, y=som_grid_columns, input_len=13, sigma=sigma, learning_rate=learning_rate)\n",
    "#********************#\n",
    "\n",
    "# Initializing the weights\n",
    "som.random_weights_init(X.values)\n",
    "\n",
    "# Training\n",
    "som.train_random(data=X.values, num_iteration=iterations)\n",
    "#********************#\n",
    "\n",
    "# Returns the distance map from the weights:\n",
    "som.distance_map()\n",
    "\n",
    "from pylab import plot, axis, show, pcolor, colorbar, bone\n",
    "\n",
    "bone()\n",
    "pcolor(som.distance_map().T)       # Distance map as background\n",
    "colorbar()\n",
    "show()\n",
    "bone()\n",
    "pcolor(som.distance_map().T)\n",
    "colorbar() #gives legend\n",
    "\n",
    "markers = ['o', 's']                 # if the observation is fraud then red circular color or else green square\n",
    "colors = ['r', 'g']\n",
    "\n",
    "for i, x in enumerate(X.values):\n",
    "    w = som.winner(x)\n",
    "    plot(w[0] + 0.5,\n",
    "         w[1] + 0.5,\n",
    "         markers[y[i]],\n",
    "         markeredgecolor = colors[y[i]],\n",
    "         markerfacecolor = 'None',\n",
    "         markersize = 10,\n",
    "         markeredgewidth = 2)\n",
    "\n",
    "show()\n",
    "\n",
    "# Store som win_map into a variable named mappings\n",
    "mappings = som.win_map(X.values)\n",
    "#********************#\n",
    "\n",
    "print(\"Number of neurons in the winning mapping:\", len(mappings.keys()))\n",
    "#********************#\n",
    "\n",
    "mappings[(9,8)]\n",
    "frauds = np.concatenate((mappings[(0,9)], mappings[(8,9)]), axis = 0)\n",
    "frauds\n",
    "\n",
    "# Convert the fraud customers back into original values using Standard Scaler(sc) inverse_transform\n",
    "frauds1 = sc.inverse_transform(frauds)\n",
    "frauds1 = pd.DataFrame(frauds1)\n",
    "frauds1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Loading the dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "target_names = iris.target_names\n",
    "\n",
    "# Scaling the data\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# Function for plotting the graphs\n",
    "def plot3clusters(X, title, vtitle):\n",
    "    plt.figure()\n",
    "    colors = ['navy','turquoise','darkorange']\n",
    "    for color, i, target_name in zip(colors, [0,1,2], target_names):\n",
    "        plt.scatter(X[y==i, 0], X[y==i, 1], color=color, label=target_name)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(vtitle + \"1\")\n",
    "    plt.ylabel(vtitle + \"2\")\n",
    "    plt.show()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, _, _ = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the Autoencoder model\n",
    "input_layer = Input(shape=(X.shape[1],))\n",
    "encoded_layer = Dense(2, activation='relu')(input_layer)\n",
    "decoded_layer = Dense(X.shape[1], activation='sigmoid')(encoded_layer)\n",
    "\n",
    "autoencoder = Model(input_layer, decoded_layer)\n",
    "\n",
    "# Compile the model\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = autoencoder.fit(X_train, X_train, epochs=50, batch_size=16, shuffle=True, validation_data=(X_test, X_test))\n",
    "\n",
    "# Plot the loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Train vs Validation Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Use the encoded layer to encode the training input\n",
    "encoded = autoencoder.predict(X_scaled)\n",
    "\n",
    "# Plot the encoded representation\n",
    "plot3clusters(encoded, \"Autoencoder Encoded Representation\", \"Encoded Feature \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN (MNIST DIGIT DATASET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 28, 28, 1)  # Reshape to include the channel dimension\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)  # Reshape to include the channel dimension\n",
    "\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(5, 2, i + 1)\n",
    "    plt.imshow(x_train[i].reshape(28, 28), cmap='gray')  # Reshape to display as 28x28 image\n",
    "\n",
    "# Build CNN Model\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "model = None  # Initialize model here\n",
    "\n",
    "'''\n",
    "Define batch size of 64 ,\n",
    "No of classes (interpret from data)\n",
    "Train for 5 epochs\n",
    "\n",
    "'''\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "def build_model(optimizer):\n",
    "\n",
    "    '''\n",
    "    Define a sequential model with categorical cross-entropy as the loss function consisting\n",
    "    of 2 convolution and 2 pooling layers with ReLU as the activation function followed by\n",
    "    dropout -> flatten -> dense -> dropout -> dense\n",
    "\n",
    "    Convolution layer - (kernel size = (3*3))\n",
    "    Pooling layer - (pool size = (2*2))\n",
    "\n",
    "    '''\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Call build model with ADAM and ADAGRAD Optimizer\n",
    "model_adam = build_model(optimizer='adam')\n",
    "model_adagrad = build_model(optimizer='adagrad')\n",
    "\n",
    "# Plot the model architecture\n",
    "plot_model(model_adam, to_file=\"mnist_model_adam.jpg\", show_shapes=True)\n",
    "plot_model(model_adagrad, to_file=\"mnist_model_adagrad.jpg\", show_shapes=True)\n",
    "\n",
    "# Train the model\n",
    "hist_adam = model_adam.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "hist_adagrad = model_adagrad.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "\n",
    "# Plot accuracy curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(hist_adam.history['accuracy'], label='Adam Training')\n",
    "plt.plot(hist_adam.history['val_accuracy'], label='Adam Validation')\n",
    "plt.plot(hist_adagrad.history['accuracy'], label='Adagrad Training')\n",
    "plt.plot(hist_adagrad.history['val_accuracy'], label='Adagrad Validation')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "keras.models.save_model(model_adam, \"mnist_adam.h5\", save_format=\"h5\")\n",
    "keras.models.save_model(model_adagrad, \"mnist_adagrad.h5\", save_format=\"h5\")\n",
    "\n",
    "# Load the model\n",
    "model_loaded_adam = load_model('mnist_adam.h5')\n",
    "\n",
    "# Define a function to predict and display an image\n",
    "def predict_image(model, img):\n",
    "    img = img.reshape(1, 28, 28, 1)\n",
    "    prediction = model.predict(img)\n",
    "    predicted_label = np.argmax(prediction)\n",
    "    print(f\"Predicted Label: {predicted_label}\")\n",
    "\n",
    "# Predict and display an image\n",
    "predict_image(model_loaded_adam, x_test[7])\n",
    "plt.imshow(x_test[7].reshape(28, 28), cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN (MNIST DIGIT DATASET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.models import Sequential , load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "x_train= x_train.reshape(60000,28,28,1)  # Reshape to include the channel dimension\n",
    "x_test= x_test.reshape(x_test.shape[0],28,28,1)  # Reshape to include the channel dimension\n",
    "\n",
    "input_shape=(28,28,1)\n",
    "\n",
    "y_train=keras.utils.to_categorical(y_train,10)\n",
    "\n",
    "y_test=keras.utils.to_categorical(y_test,10)\n",
    "\n",
    "x_train= x_train.astype('float32')\n",
    "x_test= x_test.astype('float32')\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(5,2,i+1)\n",
    "    plt.imshow(x_train[i].reshape(28, 28), cmap='gray')  # Reshape to display as 28x28 image\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Initialize model here\n",
    "model = build_model(optimizer='rmsprop')  # Call build_model with RMSprop optimizer\n",
    "\n",
    "def build_model(optimizer):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Call build model with RMSprop optimizer and SGD optimizer\n",
    "model_rmsprop = build_model(optimizer='rmsprop')\n",
    "model_sgd = build_model(optimizer='sgd')\n",
    "\n",
    "# Plot the model architecture\n",
    "plot_model(model_rmsprop, to_file=\"mnist_model_rmsprop.jpg\", show_shapes=True)\n",
    "plot_model(model_sgd, to_file=\"mnist_model_sgd.jpg\", show_shapes=True)\n",
    "\n",
    "# Train the model\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "hist_rmsprop = model_rmsprop.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "hist_sgd = model_sgd.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "\n",
    "# Save the model\n",
    "keras.models.save_model(model_rmsprop, \"mnist_rmsprop.h5\", save_format=\"h5\")\n",
    "keras.models.save_model(model_sgd, \"mnist_sgd.h5\", save_format=\"h5\")\n",
    "\n",
    "# Plot accuracy curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(hist_rmsprop.history['accuracy'], label='RMSprop Training')\n",
    "plt.plot(hist_rmsprop.history['val_accuracy'], label='RMSprop Validation')\n",
    "plt.plot(hist_sgd.history['accuracy'], label='SGD Training')\n",
    "plt.plot(hist_sgd.history['val_accuracy'], label='SGD Validation')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Load the model\n",
    "model_loaded_rmsprop = load_model('mnist_rmsprop.h5')\n",
    "\n",
    "# Define a function to predict and display an image\n",
    "def predict_image(model, img):\n",
    "    img = img.reshape(1, 28, 28, 1)\n",
    "    prediction = model.predict(img)\n",
    "    predicted_label = np.argmax(prediction)\n",
    "    print(f\"Predicted Label: {predicted_label} - {labels[predicted_label]}\")\n",
    "\n",
    "# Predict and display an image\n",
    "predict_image(model_loaded_rmsprop, x_test[7])\n",
    "plt.imshow(x_test[7].reshape(28, 28), cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "top_words = 5000\n",
    "\n",
    "# Load the IMDB Movie Review dataset\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "\n",
    "# truncate and/or pad input sequences\n",
    "max_review_length = 400\n",
    "X_train = pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train[1])\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create the model\n",
    "embedding_vector_length = 32\n",
    "\n",
    "# Create LSTM Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
    "model.add(LSTM(10))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "hist = model.fit(X_train, y_train, epochs=10, batch_size=256, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print Accuracy achieved by the model\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1] * 100))\n",
    "\n",
    "# Plot graph between epoch vs Accuracy\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot graph between epoch vs Loss\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Make predictions on test data\n",
    "pred = model.predict(X_test)\n",
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Loading the dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "print(\"X:\", X[0])\n",
    "target_names = iris.target_names\n",
    "\n",
    "# Scaling the data\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# Define a function for plotting graphs\n",
    "def plot3clusters(X, title, vtitle):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(vtitle + \" 1\")\n",
    "    plt.ylabel(vtitle + \" 2\")\n",
    "    plt.show()\n",
    "\n",
    "# Implement and visualize PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Display new reduced dimension values\n",
    "reduced_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
    "print(\"Reduced Dimension Values:\")\n",
    "print(reduced_df.head())\n",
    "\n",
    "# Plot the results\n",
    "plot3clusters(X_pca, \"PCA of Iris Dataset\", \"Principal Component\")\n",
    "\n",
    "# Display explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "print(\"Explained Variance Ratio:\")\n",
    "print(explained_variance_ratio)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
